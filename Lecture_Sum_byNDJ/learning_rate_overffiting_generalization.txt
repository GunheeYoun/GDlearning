learning_rate = 0.001
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

learning rate를 잘 정하는 것이 중요.
한발짝 step이 큰 경우 어떤 일이 발생할까.
-> step이 뛰게 되면, 반대로 왔다갔다 반복하는 일이 발생할 것.(Overshooting)
-> learning rate를 정했지만, cost가 줄어들지 않는 경우 -> overshooting 발생 가능성 및 learning rate 수정 필요
한발짝 step이 매우 작은 경우 어떤 일이 발생할까.
-> 최저점이 아닌 경우에 최저점을 설정하는 경우가 발생.

learning rate를 어떻게 정하냐에 대한 기준
-> cost function의 값을 관찰하는 것이 좋다.

Data Preprocessing for gradient Descent(경사하강법을 위한 선처리)
X 데이터를 선처리할 이유가 있다.
-> Gradient Descent Algorithm(U자형 그래프 가로-w,세로-cost)

데이터 값 간 차이가 큰 경우 Normalize할 필요가 있다.
Original Data -> Zero-centered data -> normalized Data
(데이터 값 간 차이가 어떤 범위 내에 들어가도록 한다 -> Normalize)

표준화(Standardization) (mean은 평균, std는 분산)
X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()

Overfitting(가장 큰 문제)
-> 학습을 통한 모델 생성에서 너무 잘 맞는 데이터들만 넣는 경우, 실제로는 잘 맞지 않는 경우가 많이 발생.
-> 모델을 구분하는 그래프들이 선형에서 벗어나는 경우들.
해결책 -> Training Data Set을 많이 가지는 것, feature의 개수를 줄이기(중복되는 것들),Regularization(일반화)

Regularization(Overfitting에 대한 technic)
weight값들을 너무 크게 하지 말자.

loss = 시그마(D(S(WX+b),L)/N + variation_strength * 시그마(W^2)
l2reg = 0.001 * tf.reduce_sum(tf.square(W))

data -> ML Model -> 결과(어떻게 평가를 할 까)
training set을 통한 모델 학습 -> training set에 test 데이터들로 질문.

test set은 training set과 철저히 구분되야 한다.(중복이 있으면 안됨)

Original set은 Training set과 Testing set을 포함하고 있다.
Training set은 기존 Training에 필요한 데이터와 Validation에 필요한 데이터로 나뉜다.

실제 데이터 Set (MINIST Dataset)
-> 손으로 숫자를 썻을 때 숫자 이미지를 모델이 제대로 인식하는지.
-> training set(이미지, label), testing set(이미지,label)로 구분.

정확도(모델이 얼마나 정확한지)

Lab07 (Non-normalized inputs)
학습을 진행할 수록 결과값들이 nan으로 나오는 문제.
input data들이 정규화 되지 않았기 때문.
-> 해결책(Normalized inputs으로 바꾸던지[min-max scale])

min-max scale -> 각각의 데이터 값들을 0~1 사이의 소수값으로 바꿈.

데이터들이 크거나 값들이 들쭉날쭉하는 경우 min-max scale을 사용해서 정규화하는 것이 좋다.

Lab07-2(MNIST Dataset)
X = tf.placeholder(tf.float32, [none, 이미지사이즈])
Y = tf.placeholder(tf.float32, [none, nb_classes])

데이터 몇개씩 학습시킬가 -> batch
전체 Dataset을 한번 다 학습시킨것 -> 1 epoch

1 epoch = 전체 데이터셋을 트레이닝 셋으로 한번 학습한것.
1 epoch에서 짤라서 100개씩 학습 -> batch_size
ex. 1000 training examples, batch size -> 500, 2 iterations -> 1 epoch

epoch와 batch간 이중 for loop(Training epoch/batch)

Training Data는 학습할 때만 사용.
Accuracy를 평가할 때는 Testing data들만 사용한다.

ex. accuracy.eval(session=sess, feed_dict={X:mnist.test.images, Y:mnist.test.labels})

epoch을 지날 수록, cost가 줄어든다.

재미를 더하기 위해, Sample Image show and prediction.


