Conv -> ReLU -> POOL 과정을 여러번 하여, labeling 설정.

32 * 32 * 3 image (가로 : 32, 세로 : 32, RGB Color)
이미지의 일부분만 처리하고 싶다 -> filter라는 개념.
5 * 5 * 3 filter (filter 크기는 사용자 정의가 가능)
filter는 궁극적으로 한 값을 만들어낸다.(하나의 값을 생성)

One Number = XW + b -> ReLU(XW+b)
필터를 이미지 위/왼쪽으로부터 오른쪽으로 순차적으로 이동.
같은 filter를 가지고 이미지 탐색.

여기서 중요한 것, How many numbers can we get?
Output size : (N-F)/stride + 1

하나의 영역에 다른 filter를 적용한 여러 이미지들을 합친다.
6개의 filter를 적용 후 weight이 다르게 되는데, convolution layer에 적용해서 activation map을 구성한다.
activation maps(?,?,6)
padding을 하지 않는다고 가정.
32 * 32 *3 image -> 5 * 5 * 3 6개의 filter 사용 -> (28,28,6) 가로 - 28, 세로 - 28 깊이 - 6을 가지는 Convolution layer를 형성.
28 * 28 * 6 layers -> 5 * 5 * 6 10개의 filter를 사용 -> (24, 24, 10)  가로 - 24, 세로 - 24 깊이 - 10을 가지는 Convolution layer를 형성.

activation map에 conv, ReLU를 적용. 10개의 다른 filter를 사용.

weight에 사용되는 변수의 개수는 몇개? 5 * 5 * 3 * 6 (맨 처음 과정에서)
처음에는 random하게 초기화. 우리가 가진 데이터로 학습.

CNN Introduction :  Max pooling and others
Conv -> ReLU -> POOL (반복)

Pooling Layer(Sampling)
- 이미지를 이용해서 Convolution layer를 형성. 여기서 한 개의 layer만 뽑아냄.
- 한 개의 layer를 resize하도록 한다. (sampling)
- sampling 한 결과들을 모아서 layer들을 합쳐서 다음 단계로 보낸다.

MAX POOLING
1 1  | 2 4
5 6 | 7 8
-----------
3 2 | 1 0
1 2 | 3 4
위의 4 &4 depth slice에서 2 *2 filter를 적용해서 각 filter 영역 내에서 가장 큰 값들만 모아서 sampling을 진행한다.
6 8
3 4
max pool with 2 * 2 filters and stride 2 결과는 위와 같다.
conv -> relu -> pool 순서가 반드시 고정적일 필요는 없다. (conv -> pool 로 넘어가도 됨)
https://cs.stanford.edu/people/karpathy/convnetjs/

CNN Case Study
AlexNet의 경우에서, Normalization Layer를 사용했지만. 실제로는 정규화 레이어를 사용하지 않아도 큰 차이를 보이지는 않았음.
AlexNet
- ReLU의 사용
- dropout도 중간에 사용
- batchsize 128
- 7 CNN Ensemble : 18.2 -> 15.4% 로 에러율이 낮춰짐.
GoogLeNet
ResNet (3.6% top 5 error)
- 에러율이 가장 낮았던 네트워크 설계 방식.
- AlexNet에서 8개의 레이어 사용
- ResNet에서는 152개의 레이어를 사용.
- 2~3주의 8개의 GPU 머신을 돌려서 학습을 진행.(fast forward라는 개념을 사용)
- 레이어 중간에 건너뛰어서 forward를 진행.

Plaint net vs Residual net
Plaint net -> ( x -> weight layer 통과 -> relu ->  weight layer 통과 -> relu -> H(x))
Residual net ->  ( x -> weight layer 통과 하기 전에 2개의 레이어를 건너뜀 -> H(x) = F(x) + x)


